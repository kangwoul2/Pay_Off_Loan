"""
í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
- ì›ë³¸ ë°ì´í„° ë°±ì—…(Raw Data Logging) ê¸°ëŠ¥ ì¶”ê°€
- ë‹¨ê³„ë³„ í”„ë¡œì„¸ìŠ¤ ê°€ì‹œì„± í™•ë³´
"""
import logging
import pandas as pd
import json
import os
import sys
from datetime import datetime

from .config import CrawlingConfig
from .cleansing import LoanDataCleaner
from .supabase_client import SupabaseManager
from .bank_crawlers.kb_crawler import KBCrawler

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler(f'crawling_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 60)
    logger.info("ğŸš€ ëŒ€ì¶œ ìƒí’ˆ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ë° ë°ì´í„° íŒŒíŠ¸ë¼ì¸ ì‹œì‘")
    logger.info("=" * 60)
    
    # 0. í™˜ê²½ ì¤€ë¹„: ì›ë³¸ ë°ì´í„° ì €ì¥ í´ë” ìƒì„±
    raw_data_dir = "./crawling/raw_data"
    os.makedirs(raw_data_dir, exist_ok=True)
    
    # 1. Supabase ì—°ê²°
    try:
        supabase = SupabaseManager()
        logger.info("âœ… Supabase ì—°ê²° ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    # 2. í¬ë¡¤ë§ ìˆ˜í–‰ (Extraction)
    crawlers = [KBCrawler()]
    all_raw_products = []
    
    for crawler in crawlers:
        try:
            logger.info(f"\n[STEP 1] {crawler.bank_name} ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
            products = crawler.safe_crawl()
            
            if products:
                all_raw_products.extend(products)
                logger.info(f"âœ¨ {crawler.bank_name} ì¶”ì¶œ ì™„ë£Œ: {len(products)}ê±´")
            else:
                logger.warning(f"âš ï¸ {crawler.bank_name} ì¶”ì¶œëœ ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            logger.error(f"âŒ {crawler.bank_name} ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

    if not all_raw_products:
        logger.error("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìµœì¢…ì ìœ¼ë¡œ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 3. ì›ë³¸ ë°ì´í„° ë°±ì—… (Raw Data Storage)
    # ì •ì œ ê³¼ì •ì—ì„œ ì†ì‹¤ë˜ëŠ” ë°ì´í„°ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ë°±ì—…ì…ë‹ˆë‹¤.
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    raw_file_path = f"{raw_data_dir}/raw_kb_data_{timestamp}.json"
    with open(raw_file_path, 'w', encoding='utf-8') as f:
        json.dump(all_raw_products, f, ensure_ascii=False, indent=4)
    logger.info(f"\n[STEP 2] ì›ë³¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {raw_file_path}")

    # 4. ë°ì´í„° ì •ì œ (Transformation)
    logger.info(f"\n[STEP 3] ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¬¸ì œ ì§„ë‹¨ ì‹œì‘")
    df_raw = pd.DataFrame(all_raw_products)
    
    # ì „ì²˜ë¦¬ ìƒì„¸ ë¡œì§ ê°€ë™
    cleaned_df = LoanDataCleaner.validate_and_clean(df_raw)
    
    # 5. ìµœì¢… DB ì ì¬ (Load)
    if not cleaned_df.empty:
        logger.info(f"\n[STEP 4] Supabase ë°ì´í„° ì ì¬ ë‹¨ê³„")
        products_to_insert = cleaned_df.to_dict('records')
        
        if supabase.insert_loan_products(products_to_insert):
            logger.info(f"âœ… ìµœì¢… {len(products_to_insert)}ê°œ ìƒí’ˆ DB ë°˜ì˜ ì™„ë£Œ")
            # í¬ë¡¤ë§ ì„±ê³µ ë¡œê·¸ ê¸°ë¡
            for crawler in crawlers:
                supabase.log_crawling_result(crawler.bank_name, 'success', len(cleaned_df[cleaned_df['bank_name'] == crawler.bank_name]))
        else:
            logger.error("âŒ DB ì ì¬ ì‹¤íŒ¨")
    else:
        logger.warning("âš ï¸ ì •ì œ í›„ ì ì¬í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
    logger.info("=" * 60)

if __name__ == "__main__":
    main()