"""
í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
- í™˜ê²½ ë³€ìˆ˜ ê²½ë¡œ ìµœì í™” (Root .env ë˜ëŠ” .env.local ë¡œë“œ)
- ìŠ¤í‚¤ë§ˆ ìºì‹œ ì´ìŠˆ(PGRST204) íšŒí”¼ ë¡œì§ ì ìš©
- ì›ë³¸ ë°ì´í„° ë°±ì—… ë° ë‹¨ê³„ë³„ ë¡œê¹…
"""
import logging
import pandas as pd
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# ==========================================================
# [1] í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ë¡œë“œ
# ==========================================================
base_dir = Path(__file__).resolve().parent.parent
# .envì™€ .env.local ì¤‘ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
for env_file in ['.env', '.env.local']:
    env_path = base_dir / env_file
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
        break

# ==========================================================
# ëª¨ë“ˆ ì„í¬íŠ¸ (load_dotenv ì´í›„ì— ì‹¤í–‰ë˜ì–´ì•¼ í•¨)
# ==========================================================
from .config import CrawlingConfig
from .cleansing import LoanDataCleaner
from .supabase_client import SupabaseManager
from .bank_crawlers.kb_crawler import KBCrawler

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler(f'crawling_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def main():
    logger.info("=" * 60)
    logger.info("ğŸš€ ëŒ€ì¶œ ìƒí’ˆ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ë° ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info("=" * 60)
    
    # í™˜ê²½ ë³€ìˆ˜ ì²´í¬
    if not os.getenv("SUPABASE_URL"):
        logger.error("âŒ ì—ëŸ¬: SUPABASE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    # 0. í™˜ê²½ ì¤€ë¹„: ì›ë³¸ ë°ì´í„° ì €ì¥ í´ë” ìƒì„±
    raw_data_dir = base_dir / "crawling" / "raw_data"
    os.makedirs(raw_data_dir, exist_ok=True)
    
    # 1. Supabase ì—°ê²°
    try:
        supabase = SupabaseManager()
        logger.info("âœ… Supabase ì—°ê²° ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    # 2. í¬ë¡¤ë§ ìˆ˜í–‰ (Extraction)
    crawlers = [KBCrawler()]
    all_raw_products = []
    
    for crawler in crawlers:
        try:
            logger.info(f"\n[STEP 1] {crawler.bank_name} ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
            products = crawler.safe_crawl()
            
            if products:
                all_raw_products.extend(products)
                logger.info(f"âœ¨ {crawler.bank_name} ì¶”ì¶œ ì™„ë£Œ: {len(products)}ê±´")
            else:
                logger.warning(f"âš ï¸ {crawler.bank_name} ì¶”ì¶œëœ ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            logger.error(f"âŒ {crawler.bank_name} ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

    if not all_raw_products:
        logger.error("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìµœì¢…ì ìœ¼ë¡œ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 3. ì›ë³¸ ë°ì´í„° ë°±ì—…
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    raw_file_path = raw_data_dir / f"raw_kb_data_{timestamp}.json"
    with open(raw_file_path, 'w', encoding='utf-8') as f:
        json.dump(all_raw_products, f, ensure_ascii=False, indent=4)
    logger.info(f"\n[STEP 2] ì›ë³¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {raw_file_path}")

    # 4. ë°ì´í„° ì •ì œ (Transformation)
    logger.info(f"\n[STEP 3] ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    df_raw = pd.DataFrame(all_raw_products)
    cleaned_df = LoanDataCleaner.validate_and_clean(df_raw)
    
    # 5. ìµœì¢… DB ì ì¬ (Load)
    if not cleaned_df.empty:
        logger.info(f"\n[STEP 4] Supabase ë°ì´í„° ì ì¬ ë‹¨ê³„")
        
        # ---------------------------------------------------------
        # [í•µì‹¬] PGRST204 ì—ëŸ¬ íšŒí”¼: crawled_at ì»¬ëŸ¼ì„ ë°ì´í„°ì…‹ì—ì„œ ì œê±°
        # DB ìŠ¤í‚¤ë§ˆì˜ 'DEFAULT NOW()'ê°€ ìë™ìœ¼ë¡œ ê°’ì„ ì±„ìš°ê²Œ í•©ë‹ˆë‹¤.
        # ---------------------------------------------------------
        if 'crawled_at' in cleaned_df.columns:
            cleaned_df = cleaned_df.drop(columns=['crawled_at'])
            logger.info("âš ï¸ ìŠ¤í‚¤ë§ˆ ìºì‹œ ì´ìŠˆ íšŒí”¼ë¥¼ ìœ„í•´ crawled_at í•„ë“œë¥¼ ì œì™¸í•˜ê³  ì „ì†¡í•©ë‹ˆë‹¤.")
        # ---------------------------------------------------------

        products_to_insert = cleaned_df.to_dict('records')
        
        if supabase.insert_loan_products(products_to_insert):
            logger.info(f"âœ… ìµœì¢… {len(products_to_insert)}ê°œ ìƒí’ˆ DB ë°˜ì˜ ì™„ë£Œ")
            for crawler in crawlers:
                bank_df = cleaned_df[cleaned_df['bank_name'] == crawler.bank_name]
                supabase.log_crawling_result(crawler.bank_name, 'success', len(bank_df))
        else:
            logger.error("âŒ DB ì ì¬ ì‹¤íŒ¨")
    else:
        logger.warning("âš ï¸ ì ì¬í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    logger.info("\n" + "=" * 60)
    logger.info("ğŸ íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
    logger.info("=" * 60)

if __name__ == "__main__":
    main()