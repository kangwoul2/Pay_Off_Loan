"""
KBêµ­ë¯¼ì€í–‰ í¬ë¡¤ëŸ¬ (ì‹¤ì œ HTML êµ¬ì¡° ë°˜ì˜ ë²„ì „)
"""
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import logging
from typing import List, Dict

from ..crawler import BaseBankCrawler
from ..cleansing import LoanDataCleaner

logger = logging.getLogger(__name__)

class KBCrawler(BaseBankCrawler):
    def __init__(self):
        super().__init__('KB')
        self.url = self.config.BANK_URLS['KB']
    
    def crawl(self) -> List[Dict]:
        products = []
        logger.info(f"{self.bank_name} íŽ˜ì´ì§€ ì ‘ì†: {self.url}")
        self.driver.get(self.url)
        
        # 1. ì‹¤ì œ ë°ì´í„°(area1)ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¶©ë¶„ížˆ ëŒ€ê¸°
        try:
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, "area1"))
            )
            logger.info("ðŸŽ‰ ì‹¤ì œ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ í™•ì¸!")
        except:
            logger.warning("âŒ ìƒí’ˆ ëª©ë¡(area1) ë¡œë“œ ì‹¤íŒ¨. íŒì—…ì°½ì„ í™•ì¸í•˜ê±°ë‚˜ ë”ë¯¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._create_dummy_data()
        
        # 2. HTML íŒŒì‹±
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        
        # 3. ë³´ë‚´ì£¼ì‹  êµ¬ì¡°(.area1)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒí’ˆ ì¶”ì¶œ
        product_items = soup.select('.area1')
        logger.info(f"íŒŒì‹±ëœ ì•„ì´í…œ ìˆ˜: {len(product_items)}ê°œ")

        for item in product_items:
            try:
                # ìƒí’ˆëª…: a.title ë‚´ì˜ strong íƒœê·¸
                name_tag = item.select_one('a.title strong')
                # ìš”ì•½ì •ë³´: span.msg
                msg_tag = item.select_one('span.msg')
                # í•œë„ ì •ë³´: div.info-data2
                limit_tag = item.select_one('.info-data2')
                
                if not name_tag:
                    continue

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                product_name = name_tag.get_text(strip=True)
                product_limit = limit_tag.get_text(strip=True) if limit_tag else "ìƒì„¸ë¬¸ì˜"
                
                logger.info(f"ðŸ”Ž ìƒí’ˆ ë°œê²¬: {product_name} | {product_limit}")

        
                # kb_crawler.py ë‚´ì˜ raw_data ë¶€ë¶„
                raw_data = {
                    'bank_name': self.bank_name,
                    'product_name': product_name,
                    'product_type': 'ì‹ ìš©ëŒ€ì¶œ',
                    'rate': '4.2',  # ìˆ˜ì§‘ì´ ì–´ë ¤ìš°ë©´ ì¼ë‹¨ ê³ ì • ìˆ«ìž ë¬¸ìžì—´ë¡œ ì „ë‹¬
                    'limit': product_limit.replace('ìµœê³ ', '').replace('ì–µì›', '00000000').strip(),
                    'fee': '1.5',
                    'waiver': '36'
                }
                
                # ì „ì²˜ë¦¬ (LoanDataCleanerì—ì„œ ìˆ«ìžë¡œ ë³€í™˜ ë“± ìˆ˜í–‰)
                cleaned_data = LoanDataCleaner.parse_product_row(raw_data)
                if cleaned_data:
                    products.append(cleaned_data)
                
            except Exception as e:
                logger.warning(f"ê°œë³„ ìƒí’ˆ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return products if products else self._create_dummy_data()

    def _create_dummy_data(self) -> List[Dict]:
        return [
            {
                'bank_name': 'KB',
                'product_name': 'KBìŠ¤íƒ€ ì‹ ìš©ëŒ€ì¶œ(ê°€ì§œ)',
                'product_type': 'ì‹ ìš©ëŒ€ì¶œ',
                'base_rate': 4.5,
                'additional_rate': 1.0,
                'max_limit': 100000000,
                'early_repay_fee_rate': 1.5,
                'fee_waiver_months': 36,
                'salary_transfer_discount': 0.3
            }
        ]